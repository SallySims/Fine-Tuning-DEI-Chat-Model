# Needed Packages

## Installing the Packages, if not already installed
!pip install huggingface_hub hf_transfer datasets
!pip install transformers peft accelerate trl --upgrade
!pip install torch
!pip install bitsandbytes



## Importing the packages
import pandas as pd
import numpy as np
import torch
import os
from datasets import load_dataset
from peft import LoraConfig, get_peft_model
from transformers import AutoModelForCausalLM, AutoTokenizer,TrainingArguments, pipeline, BitsAndBytesConfig
from trl import SFTTrainer

from huggingface_hub import login
login(token="ADD YOUR TOKEN")


####Token
Convo_T=" ADD YOUR TOKEN"

# Create the quantization config (use 4-bit)
quantization_config = BitsAndBytesConfig(load_in_4bit=True)

# Load Llama Model with quantization config
model = AutoModelForCausalLM.from_pretrained(
  "meta-llama/Llama-3.2-1B-Instruct",
    quantization_config=quantization_config,  # Using the new approach
    device_map="auto",  # Automatically choose device (GPU/CPU)
    trust_remote_code=True,  # Ensure the model's code is trusted
    token=Convo_T  # Your Hugging Face token for access (if required)
)


tokenizer=AutoTokenizer.from_pretrained(
    "meta-llama/Llama-3.2-1B-Instruct",
    token=Convo_T,
    trust_remote_code=True
)



### Incorporating the data for Training

#### Loading data from my drive
from google.colab import drive
drive.mount('/content/drive/')

DataConvo=pd.read_csv('/content/drive/My Drive/PATHTOYOURDATA/SallySimsforTuning.csv', encoding="ISO-8859-1")
DataConvo


## Renaming the column
DataConvo.columns=['Tweet_Content', 'About DEI and Why']## select only the columns of interest
DataConvo


### Converting the data in to Conservation for Tuning
#### Create an empty vector
ChatData=[]
#### Loop through to fill the vector
for index, row in DataConvo.iterrows():
  conversation={
      "messages":[
          {"role":"system", "content":"You are a helpful assistant"},
          {"role": "user", "content": row["Tweet_Content"]},
          {"role": "assistant", "content": row["About DEI and Why"]}
      ]
  }

#### Append yhe outomes of the loops into the emptry vector
  ChatData.append(conversation)
  ChatData

#### Saving Data to Hugging face format (JSON script)
DataChat=Dataset.from_list(ChatData)
DataChat.push_to_hub("SallySims/DEIChat")## Insert your page


### Accessing the Apply Chat Template
# Flatten the ChatData to a list of messages (to help convert this messages into a string)
flattened_messages = [message for conversation in ChatData for message in conversation['messages']]

Formatted_text = tokenizer.apply_chat_template(flattened_messages, tokenize=False, add_generation_prompt=True)
Formatted_text

### Adding Lora Adapters

from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType

model=get_peft_model(
    model,
    LoraConfig(
    r=16,
    lora_alpha=16,
    lora_dropout=0.05,
    bias="none")
)
model

######Preparing to train my data

EOS_TOKEN=tokenizer.eos_token
EOS_TOKEN


# Instead, tokenize question and answer separately and then concatenate the tokenized ids.
def preprocess_function(examples):
    convos=examples["messages"]
    # Transofrm datset using the chat template
    # Iterate through each conversation and apply the template to its messages
    texts = []
    for convo in convos:
        formatted_text = tokenizer.apply_chat_template(
            convo, tokenize=False, add_generation_prompt=False
        )
        texts.append(formatted_text)
    return{"text":texts}

## Load Data from Datasets
from datasets import load_dataset
ChatDataDEI=load_dataset("SallySims/DEIChat", split="train")
ChatDataDEI1 = ChatDataDEI.map(preprocess_function, batched=True)
ChatDataDEI1

### Testing the Data
for i, sample in enumerate(ChatDataDEI1):
  print(f"\n----- Sample{i+1}----")
  print(sample["text"])
  if i>2:
    break


### Fine Tuning The Model
from transformers import TrainingArguments
from trl import SFTTrainer
# Correct TrainingArguments setup (removed `max_seq_length`)
training_args = TrainingArguments(
    output_dir="./outputs",
    per_device_train_batch_size=2,
    gradient_accumulation_steps=4,
    num_train_epochs=5,
    warmup_steps=5,
    learning_rate=2e-4,
    weight_decay=0.01,
    logging_steps=1,
    save_strategy="no",
    fp16=not torch.cuda.is_bf16_supported(),
    bf16=torch.cuda.is_bf16_supported(),
    # No `max_seq_length` here
)

# Initialize trainer
trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=ChatDataDEI1,
)

## Training
trainer_stats=trainer.train()
trainer_stats


##### Tesing the New Model
from transformers import pipeline, AutoModelForCausalLM
messages=[
    {"role":"user", "content": "It's mental health as defined by capitalism. Only pursuits which help the economy can improve your mental health, not rigorous systems of support"},
]
inputs=tokenizer.apply_chat_template(
    messages,
    tokenize=True,
    add_generation_prompt=True, #must add for generation
    return_tensors="pt",).to("cuda")

from transformers import TextStreamer
text_streamer=TextStreamer(tokenizer)
_=model.generate(inputs,
                 streamer=text_streamer,
                 max_new_tokens=128,
                 use_cache=True)

#### Pusing to Hugguing Face
## Pushing the main model
model.push_to_hub("SallySims/DEI_Chat_Model_Lora",
                  tokenizer,
                  token="Convo_T"
                      )

## Optional
#### Pushing to HuggingFace in GGUF Format
model.push_to_hub_gguf("SallySims/DEI_Chat_Model_Lora_GGUF",
                  tokenizer,
                  quantization_method="q4_k_m",
                  token="Convo_T"
                      )
